Try out deep learning models online on Colab with a single click.

## Image Super-Resolution
* Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN). A combination of [xinntao/ESRGAN](https://github.com/xinntao/ESRGAN) and [ata4/esrgan-launcher](https://github.com/ata4/esrgan-launcher). My colab fork is located in [styler00dollar/Colab-ESRGAN](https://github.com/styler00dollar/Colab-ESRGAN). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-ESRGAN/blob/master/Colab-ESRGAN-(new&old-arch).ipynb)
  * Train ESRGAN with forked [xinntao/BasicSR](https://github.com/xinntao/BasicSR) ([victorca25/BasicSR](https://github.com/victorca25/BasicSR)) and my fork [styler00dollar/Colab-BasicSR](https://github.com/styler00dollar/Colab-BasicSR) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-BasicSR/blob/master/Colab_BasicSR.ipynb) or with [DinJerr/BasicSR](https://github.com/DinJerr/BasicSR.git) by using [Sunseille/Colab_Toolbox](https://github.com/Sunseille/Colab_Toolbox). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Sunseille/Colab_Toolbox/blob/master/BasicSR_Blueball.ipynb)
* Deep Unfolding Network for Image Super-Resolution (USRNet). The original repositories are [cszn/USRNet](https://github.com/cszn/USRNet) and [cszn/KAIR](https://github.com/cszn/KAIR). My colab fork is located in [styler00dollar/Colab-USRNet](https://github.com/styler00dollar/Colab-USRNet). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-USRNet/blob/master/Colab-USRNet.ipynb)
* Image Super Resolution with [idealo/image-super-resolution](https://github.com/idealo/image-super-resolution). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/ISR_Prediction_Tutorial.ipynb)
* PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1-cyGV0FoSrHcQSVq3gKOymGTMt0g63Xc?usp=sharing#sandboxMode=true) or [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tg-bomze/Face-Depixelizer/blob/master/Face_Depixelizer_Eng.ipynb)
* SPSR: Structure-Preserving Super Resolution with Gradient Guidance with [Maclory/SPSR](https://github.com/Maclory/SPSR). A Colab based on [BlueAmulet/SPSR](https://github.com/BlueAmulet/SPSR) is located in my fork [styler00dollar/Colab-SPSR](https://github.com/styler00dollar/Colab-SPSR). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-SPSR/blob/master/Colab-SPSR.ipynb)
* Blind Face Restoration via Deep Multi-scale Component Dictionaries with [csxmli2016/DFDNet](https://github.com/csxmli2016/DFDNet). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tg-bomze/DFDNet/blob/whole/DFDNet_Colab.ipynb)
  * Alternative Colab provided by [xinntao/BasicSR](https://github.com/xinntao/BasicSR). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1RoNDeipp9yPjI3EbpEbUhn66k5Uzg4n8)

## Video Interpolation
* RIFE: Real-Time Intermediate Flow Estimation for Video Frame Interpolation with [hzwer/arXiv2020-RIFE](https://github.com/hzwer/arXiv2020-RIFE). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hzwer/arXiv2020-RIFE/blob/main/Colab_demo.ipynb) My fork with FFMPEG is located in [styler00dollar/Colab-RIFE](https://github.com/styler00dollar/Colab-RIFE). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-RIFE/blob/main/Colab-RIFE-FFMPEG.ipynb)
* Depth-Aware Video Frame Interpolation (DAIN) using [baowenbo/DAIN](https://github.com/baowenbo/DAIN). My fork is located in [styler00dollar/Colab-DAIN](https://github.com/styler00dollar/Colab-DAIN). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DAIN/blob/master/Colab_DAIN.ipynb)
  * DAIN NCNN with [nihui/dain-ncnn-vulkan](https://github.com/nihui/dain-ncnn-vulkan). My Colab fork is located in [styler00dollar/Colab-dain-ncnn-vulkan](https://github.com/styler00dollar/Colab-dain-ncnn-vulkan). [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-dain-ncnn-vulkan/blob/master/Colab-dain-ncnn-vulkan.ipynb)
* CAIN NCNN (Channel Attention Is All You Need for Video Frame Interpolation) with [nihui/cain-ncnn-vulkan](https://github.com/nihui/cain-ncnn-vulkan). My fork is located in [styler00dollar/Colab-cain-ncnn-vulkan](https://github.com/styler00dollar/Colab-cain-ncnn-vulkan). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-cain-ncnn-vulkan/blob/master/Colab-cain-vulkan.ipynb)
* High Quality Estimation of Multiple Intermediate Frames for Video Interpolation with [avinashpaliwal/Super-SloMo](https://github.com/avinashpaliwal/Super-SloMo). My colab fork is locaced in [styler00dollar/Colab-Super-SloMo](https://github.com/styler00dollar/Colab-Super-SloMo). My version: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-Super-SloMo/blob/master/Colab-Super-SloMo.ipynb) The original: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/SuperSloMo.ipynb)
* Featureflow can be found in [CM-BF/FeatureFlow](https://github.com/CM-BF/FeatureFlow). The colab was made by Mr. Anon. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1eg-ApEMzYRGUoacEtnxMWN9lESw_0E1C?usp=sharing)
* Video Frame Interpolation via Residue Refinement with [HopLee6/RRIN](https://github.com/HopLee6/RRIN). My fork is located in [styler00dollar/Colab-RRIN](https://github.com/styler00dollar/Colab-RRIN). [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-RRIN/blob/master/Colab-RRIN.ipynb)

## Video Super-Resolution
* Upscale Videos with [open-mmlab/mmsr](https://github.com/open-mmlab/mmsr) and this colab repo [Sunseille/Colab_Toolbox](https://github.com/Sunseille/Colab_Toolbox). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Sunseille/Colab_Toolbox/blob/master/MMSR%20Colab%20Toolbox.ipynb)


## Video Super-Resolution and Interpolation
* Fast and Accurate One-Stage Space-Time Video Super-Resolution with [Mukosame/Zooming-Slow-Mo-CVPR-2020](https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020). My colab fork is located in [styler00dollar/Colab-Zooming-Slow-Mo](https://github.com/styler00dollar/Colab-Zooming-Slow-Mo). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-Zooming-Slow-Mo/blob/master/Colab-Zooming-Slow-Mo.ipynb)

## Inpainting
* DFNet: Deep Fusion Network for Image completion with [hughplay/DFNet](https://github.com/hughplay/DFNet). My fork with [Yukariin/DFNet](https://github.com/Yukariin/DFNet) is located in [styler00dollar/Colab-DFNet](https://github.com/styler00dollar/Colab-DFNet). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DFNet/blob/master/Colab-DFNet.ipynb)
* [open-mmlab/mmediting](https://github.com/open-mmlab/mmediting) is an open source image and video editing toolbox based on PyTorch. My fork is located in [styler00dollar/Colab-mmediting](https://github.com/styler00dollar/Colab-mmediting). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-mmediting/blob/master/Colab-mmediting.ipynb)
* EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning with [knazeri/edge-connect](https://github.com/knazeri/edge-connect). My fork is located in [styler00dollar/Colab-edge-connect](https://github.com/styler00dollar/Colab-edge-connect). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-edge-connect/blob/master/Colab-edge-connect.ipynb)
* Rethinking Inpainting with [KumapowerLIU/Rethinking-Inpainting-MEDFE](https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE). My fork is located in [styler00dollar/Colab-MEDFE](https://github.com/styler00dollar/Colab-MEDFE). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-MEDFE/blob/master/Colab-MEDFE.ipynb) **[WARNING: NEEDS MATLAB]**
* Region Normalization for Image Inpainting with [geekyutao/RN](https://github.com/geekyutao/RN). My fork is located in [styler00dollar/Colab-RN](https://github.com/styler00dollar/Colab-RN). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-RN/blob/master/Colab-RN.ipynb)
* Coherent Semantic Attention Image Inpainting with [Yukariin/CSA_pytorch](https://github.com/Yukariin/CSA_pytorch). My fork is located in [styler00dollar/Colab-CSA-pytorch](https://github.com/styler00dollar/Colab-CSA-pytorch). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-CSA-pytorch/blob/master/Colab-CSA-pytorch.ipynb)
* Pluralistic Image Completion with [lyndonzheng/Pluralistic-Inpainting](https://github.com/lyndonzheng/Pluralistic-Inpainting). My fork is located in [styler00dollar/Colab-Pluralistic-Inpainting](https://github.com/styler00dollar/Colab-Pluralistic-Inpainting). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-Pluralistic-Inpainting/blob/master/Colab-Pluralistic-Inpainting.ipynb)
* [SayedNadim/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting](https://github.com/SayedNadim/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting). My fork is located in [styler00dollar/Colab-Global-and-Local-Inpainting](https://github.com/styler00dollar/Colab-Global-and-Local-Inpainting). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-Global-and-Local-Inpainting/blob/master/Colab-Global-and-Local-Inpainting.ipynb)
* [JiahuiYu/generative_inpainting](https://github.com/JiahuiYu/generative_inpainting) aka DeepFill v1/v2 with Contextual Attention and Gated Convolution. My fork is located in [styler00dollar/Colab-generative-inpainting](https://github.com/styler00dollar/Colab-generative-inpainting). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-generative-inpainting/blob/master/Colab-deepfillv2.ipynb)
* Video inpainting with Flow-edge Guided Video Completion can be found in [vt-vl-lab/FGVC](https://github.com/vt-vl-lab/FGVC). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1pb6FjWdwq_q445rG2NP0dubw7LKNUkqc?usp=sharing)

## TTS
* LibriTTS trained multi speaker TTS demo using [NVIDIA/flowtron](https://github.com/NVIDIA/flowtron).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidia_Flowtron_Waveglow.ipynb)
* An English female voice ([LJSpeech](https://keithito.com/LJ-Speech-Dataset/)) demo using [Rayhane-mamah/Tacotron-2](https://github.com/Rayhane-mamah/Tacotron-2) and [r9y9/wavenet_vocoder](https://github.com/r9y9/wavenet_vocoder)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/Tacotron2_and_WaveNet_text_to_speech_demo.ipynb)
* A Mongolian male voice demo using [Rayhane-mamah/Tacotron-2](https://github.com/Rayhane-mamah/Tacotron-2) with the Griffin-Lim algorithm.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/mongolian-nlp/blob/master/misc/Tacotron_MongolianTTS.ipynb)  
* An English female voice ([LJSpeech](https://keithito.com/LJ-Speech-Dataset/)) demo using [tugstugi/pytorch-dc-tts](https://github.com/tugstugi/pytorch-dc-tts) with the Griffin-Lim algorithm.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/pytorch-dc-tts/blob/master/notebooks/EnglishTTS.ipynb)
* An English female voice ([LJSpeech](https://keithito.com/LJ-Speech-Dataset/)) demo using [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) (Tacotron + WaveRNN).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/fatchordWaveRNN.ipynb)
* An English female voice ([LJSpeech](https://keithito.com/LJ-Speech-Dataset/)) demo using [mozilla/TTS](https://github.com/mozilla/TTS) (Tacotron + WaveRNN).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/Mozilla_TTS_WaveRNN.ipynb)
* [NVIDIA/mellotron](https://github.com/NVIDIA/mellotron) notebook.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yhgon/mellotron/blob/master/inference_colab.ipynb)
* Voice clone demo using [CorentinJ/Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/RealTimeVoiceCloning.ipynb)
* Official [ESPnet](https://github.com/espnet/espnet) English/Chinese/Japanese TTS notebook. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb)
* Official [ForwardTacotron](https://github.com/as-ideas/ForwardTacotron) LJSpeech TTS notebook. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/as-ideas/ForwardTacotron/blob/master/notebooks/synthesize.ipynb)

## Speech Recognition
* [mozilla/DeepSpeech](https://github.com/mozilla/DeepSpeech) with LM on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/MozillaDeepSpeech.ipynb)
* Wav2Letter+ from [NVIDIA/OpenSeq2Seq](https://github.com/NVIDIA/OpenSeq2Seq.git) without LM on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidiaWav2LetterPlus.ipynb)
* Jasper from [NVIDIA/OpenSeq2Seq](https://github.com/NVIDIA/OpenSeq2Seq.git) without LM on Youtube videos. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidiaJasper.ipynb)
* QuartzNet from [NVIDIA/Nemo](https://github.com/NVIDIA/NeMo.git) without LM on Youtube videos. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidiaQuartzNet.ipynb)
* QuartzNet from [NVIDIA/Nemo](https://github.com/NVIDIA/NeMo.git) without LM with microphone. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidiaQuartzNetMic.ipynb)
* Official [ESPnet](https://github.com/espnet/espnet) Spanish->English speech translation notebook. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/st_demo.ipynb)

## Object Detection
* Tensorflow object detection: FasterRCNN+InceptionResNet and ssd+mobilenet.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb)
* Cascade RCNN demo using [open-mmlab/mmdetection](https://github.com/open-mmlab/mmdetection).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/Open_MMLab_Detection_Toolbox_Cascade_RCNN.ipynb)
* YOLO v3 demo using [ayooshkathuria/pytorch-yolo-v3](https://github.com/ayooshkathuria/pytorch-yolo-v3).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/YOLOv3_PyTorch.ipynb)
* YOLO v4 with [AlexeyAB/darknet](https://github.com/AlexeyAB/darknet). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/12QusaaRj_lUwCGDvQNfICpa7kA7_a2dE)
* YOLO v5 with [ultralytics/yolov5](https://github.com/ultralytics/yolov5). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb)
* Object detection on Youtube videos using [amdegroot/ssd.pytorch](https://github.com/amdegroot/ssd.pytorch) (SSD300).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/SSD_Pytorch_Video.ipynb)  
* CenterNet (Objects as Points) demo using [xingyizhou/CenterNet](https://github.com/xingyizhou/CenterNet).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/CenterNet_ObjectsAsPoints.ipynb)
* Official DE⫶TR demo notebook [facebookresearch/detr](https://github.com/facebookresearch/detr).
  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb)
* Official Google [EfficientDet](https://arxiv.org/abs/1911.09070) notebook.
  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/automl/blob/master/efficientdet/tutorial.ipynb)
* Test and train box-models from [Tensorflow detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) with [dctian/DeepPiCar](https://github.com/dctian/DeepPiCar). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dctian/DeepPiCar/blob/master/models/object_detection/code/tensorflow_traffic_sign_detection.ipynb)

## Segmentation
* Semantic segmentation trained on [ADE20K](http://groups.csail.mit.edu/vision/datasets/ADE20K/) using [CSAILVision/semantic-segmentation-pytorch](https://github.com/CSAILVision/semantic-segmentation-pytorch).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/CSAILVision_SemanticSegmentation.ipynb)
* [DeepLabV3](https://arxiv.org/abs/1706.05587) from [torchvision](https://pytorch.org/docs/stable/torchvision/index.html).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/TorchvisionDeepLabV3.ipynb)
* Fast tracking and segmentation with [SiamMask](https://github.com/foolwood/SiamMask) on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/SiamMask.ipynb)
* Real-time semantic segmentation with [LightNet++](https://github.com/ansleliu/LightNetPlusPlus) on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/LightNetPlusPlus.ipynb)
* Real-time instance segmentation with [YOLACT](https://github.com/dbolya/yolact) on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/YOLACT.ipynb)
* Instance segmentation with [CenterMask](https://github.com/youngwanLEE/CenterMask/).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/CenterMask.ipynb)
* Train and test [Tensorflow detection model zoo mask models](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) with [TannerGilbert/Tensorflow-Object-Detection-API-train-custom-Mask-R-CNN-model](https://github.com/TannerGilbert/Tensorflow-Object-Detection-API-train-custom-Mask-R-CNN-model). There is also [a tutorial](https://gilberttanner.com/blog/train-a-mask-r-cnn-model-with-the-tensorflow-object-detection-api) dedicated to this repo. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/TannerGilbert/Tensorflow-Object-Detection-API-train-custom-Mask-R-CNN-model/blob/master/Tensorflow_Object_Detection_API_Instance_Segmentation_in_Google_Colab.ipynb)
* Open source semantic segmentation toolbox [open-mmlab/mmsegmentation](https://github.com/open-mmlab/mmsegmentation). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/open-mmlab/mmsegmentation/blob/master/demo/MMSegmentation_Tutorial.ipynb)
* Mask RCNN demo using [matterport/Mask_RCNN](https://github.com/matterport/Mask_RCNN).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/Matterport_Mask_RCNN.ipynb)
* Mask RCNN demo using [Detectron](https://github.com/facebookresearch/Detectron).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/Detectron_MaskRCNN.ipynb)
* Detectron2:
  * Official Detectron2 Mask RCNN demo with [facebookresearch/detectron2](https://github.com/facebookresearch/detectron2).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5)
  * A combination of [facebookresearch/detectron2](https://github.com/facebookresearch/detectron2), [zhanghang1989/detectron2-ResNeSt](https://github.com/zhanghang1989/detectron2-ResNeSt) and [youngwanLEE/centermask2](https://github.com/youngwanLEE/centermask2) with my fork [styler00dollar/Colab-Detectron2](https://github.com/styler00dollar/Colab-Detectron2): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-Detectron2-with-Original-and-ResNeSt/blob/resnest/Colab-Detectron2-(Original%2BResNeSt).ipynb)
* Mask RCNN demo from [torchvision](https://pytorch.org/docs/stable/torchvision/index.html).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/TorchvisionMaskRCNN.ipynb)
* Example usage of [open-mmlab/mmdetection](https://github.com/open-mmlab/mmdetection) with my fork [styler00dollar/Colab-mmdetection](https://github.com/styler00dollar/Colab-mmdetection). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-mmdetection/blob/master/Colab-mmdetection.)

## Multi Object Tracking
* Pedestrian tracking using [ZQPei/deep_sort_pytorch](https://github.com/ZQPei/deep_sort_pytorch) (DeepSORT + YOLOv3).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/DeepSORT_YOLOv3.ipynb)  

## Pose Detection
* [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/OpenPose.ipynb)
* [AlphaPose](https://github.com/MVIG-SJTU/AlphaPose) on Youtube videos.
  * v0.2.0: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/AlphaPose.ipynb)
  * v0.3.0: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/AlphaPoseV0_3_0.ipynb)
* [DensePose](https://github.com/facebookresearch/DensePose) demo notebook.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/DensePose.ipynb)
* HRNet using [lxy5513/hrnet](https://github.com/lxy5513/hrnet) on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/HRNet_lxy5513.ipynb)
* Keypoint R-CNN from [torchvision](https://pytorch.org/docs/stable/torchvision/index.html).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/TorchvisionPersonKeypoint.ipynb)

## Scene Text Detection
* [PixelLink](https://github.com/ZJULearning/pixel_link) demo notebook.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/PixelLink.ipynb)
* Scene text detection using [argman/EAST](https://github.com/argman/EAST).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/EAST.ipynb)
* Scene text detection using [CRAFT-pytorch](https://github.com/clovaai/CRAFT-pytorch).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/CRAFT.ipynb)

## Image generation
* BigGAN:
  * BigGAN [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb)
  * Text-based image generation with BigGAN and CLIP:
    * Colab by [@advadnoun](https://twitter.com/advadnoun/status/1351038053033406468). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing#scrollTo=reChJi_vrNI_)
    * A modification of that notebook to reduce amount of text/clicks can be found inside [styler00dollar/Colab-BigGANxCLIP](https://github.com/styler00dollar/Colab-BigGANxCLIP). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-BigGANxCLIP/blob/main/Colab-BigGANxCLIP.ipynb)
    * Colab by [@eyaler](https://twitter.com/eyaler/status/1351044325392719876) / [eyaler/clip_biggan](https://github.com/eyaler/clip_biggan). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eyaler/clip_biggan/blob/main/ClipBigGAN.ipynb)
    * Alternative Colab with CMA-ES is also inside [eyaler/clip_biggan](https://github.com/eyaler/clip_biggan). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eyaler/clip_biggan/blob/main/WanderCLIP.ipynb)
* StyleGAN2:
  * StyleGAN2 with Differentiable Augmentation with [mit-han-lab/data-efficient-gans](https://github.com/mit-han-lab/data-efficient-gans). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/zsyzzsoft/5fbb71b9bf9a3217576bebae5de46fc2/data-efficient-gans.ipynb#scrollTo=Re5R6VX8VNgo) 
  * Style-based GAN architecture (StyleGAN2) can be found in [NVlabs/stylegan2](https://github.com/NVlabs/stylegan2). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/parthsuresh/stylegan2-colab/blob/master/StyleGAN2_Google_Colab.ipynb)
  * Anime+StyleGAN2:
    * Style-based GAN architecture (StyleGAN2) with anime face generation. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Pv8OIFlonha4KeYyY2oEFaK4mG-alaWF#scrollTo=q8VnyjDhiBQY&forceEdit=true&sandboxMode=true)
    * [Newest model training attempt by aydao (@AydaoAI) and Colab provided by arfa (@arfafax)](https://twitter.com/arfafax/status/1351604423303127040). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1oxcJ1tbG77hlggdKd_d8h22nBcIZsLTL?usp=sharing)
      * That combined with CLIP provided by [nagolinc/notebooks](https://github.com/nagolinc/notebooks). **(Warning: Results usually look bad, even had code related errors when I tested it.)** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nagolinc/notebooks/blob/main/TADNE_and_CLIP.ipynb)
      * A more compact and fixed version of that notebook by me. (Only fixed syntax errors, results still not reliable.) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17d4kfpqCJlbpuiNh4h-edyMfTfn0Hc-N?usp=sharing)
      * A V2 appeared inside the original repo [nagolinc/notebooks](https://github.com/nagolinc/notebooks), but still has quite a lot of boxes. The ``psi`` parameter is also gone. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nagolinc/notebooks/blob/main/CLIP_%2B_TADNE_(pytorch)_v2.ipynb)

* Text-based image generation with SIREN and CLIP. Original notebook from [here](https://twitter.com/advadnoun/status/1348375026697834496?s=19). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1FoHdqoqKntliaQKnMoNs3yn5EALqWtvP?usp=sharing)
  * A more compact version can be found inside [lucidrains/deep-daze](https://github.com/lucidrains/deep-daze). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1_YOHdORb0Fg1Q7vWZ_KlrtFe9Ur3pmVj?usp=sharing) 
  * My version with small modifications to [lucidrains/deep-daze](https://github.com/lucidrains/deep-daze). Less printing and Google Drive support with [styler00dollar/Colab-deep-daze](https://github.com/styler00dollar/Colab-deep-daze). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-deep-daze/blob/main/Colab-Deep-Daze.ipynb)

* Taming Transformers for High-Resolution Image Synthesis with [CompVis/taming-transformers](https://github.com/CompVis/taming-transformers). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb)
  * My fork with smaller Colab is located in [styler00dollar/Colab-taming-transformers](https://github.com/styler00dollar/Colab-taming-transformers).  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-taming-transformers/blob/master/Colab-taming-transformers.ipynb)

## Image colorization
* [DeOldify](https://github.com/jantic/DeOldify): A Deep Learning based project for colorizing and restoring old images.
  * My fork that combines deoldify anime and normal deoldify is located in [styler00dollar/Colab-DeOldify](https://github.com/styler00dollar/Colab-DeOldify). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DeOldify/blob/master/Colab-DeOldify-(Original%2BAnime).ipynb)
  * deoldify an image (artistic model) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb)
  * deoldify an image (stable model) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ColorizeTrainingStable.ipynb)
  * deoldify an anime image with [Dakini/AnimeColorDeOldify](https://github.com/Dakini/AnimeColorDeOldify). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Dakini/AnimeColorDeOldify/blob/master/ImageColorizerColab.ipynb)
* Coloring images with [pvitoria/ChromaGAN](https://github.com/pvitoria/ChromaGAN). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pvitoria/ChromaGAN/blob/master/DemoChromaGAN.ipynb)
* A lightweight genative architexture for image inpainting wih [GuardSkill/AdaptiveGAN](https://github.com/GuardSkill/AdaptiveGAN). My fork is located in [styler00dollar/Colab-AdaptiveGAN](https://github.com/styler00dollar/Colab-AdaptiveGAN). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-AdaptiveGAN/blob/master/Colab-AdaptiveGAN.ipynb)
* Instance-aware Image Colorization with [ericsujw/InstColorization](https://github.com/ericsujw/InstColorization). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericsujw/InstColorization/blob/master/InstColorization.ipynb)

## Image modification
* Restore old photos with [microsoft/Bringing-Old-Photos-Back-to-Life](https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NEm6AsybIiC5TwTU_4DqDkQO0nFRB-uA?usp=sharing)
* Decensoring Hentai with Deep Neural Networks. The original repo is [deeppomf/DeepCreamPy](https://github.com/deeppomf/DeepCreamPy) and my fork is located in [styler00dollar/Colab-DeepCreamPy](https://github.com/styler00dollar/Colab-DeepCreamPy). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DeepCreamPy/blob/master/Colab-DeepCreamPy-2.0.ipynb)
* [dreamnettech/dreampower](https://github.com/dreamnettech/dreampower) is a deep learning algorithm based on DeepNude with the ability to nudify photos of people. My fork is located in [styler00dollar/Colab-dreampower](https://github.com/styler00dollar/Colab-dreampower). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-dreampower/blob/master/Colab-Dream.ipynb)
* Deblurring pictures with [TAMU-VITA/DeblurGANv2](https://github.com/TAMU-VITA/DeblurGANv2). My own fork is located in [styler00dollar/Colab-DeblurGANv2](https://github.com/styler00dollar/Colab-DeblurGANv2). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DeblurGANv2/blob/master/Colab-DeblurGANv2.ipynb)
* Deblurring pictures with [SeungjunNah/DeepDeblur-PyTorch](https://github.com/SeungjunNah/DeepDeblur-PyTorch). My own fork is located in [styler00dollar/Colab-DeepDeblur](https://github.com/styler00dollar/Colab-DeepDeblur). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DeepDeblur/blob/master/Colab-DeepDeblur.ipynb)
* [gordicaleksa/pytorch-deepdream](https://github.com/gordicaleksa/pytorch-deepdream) will give you the power to create weird and psychedelic-looking images. My fork is located in [styler00dollar/Colab-deepdream](https://github.com/styler00dollar/Colab-deepdream). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-deepdream/blob/master/Colab-deepdream.ipynb)
* Single-Image HDR Reconstruction by Learning to Reverse the Camera Pipeline with [alex04072000/SingleHDR](https://github.com/alex04072000/SingleHDR). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WzNaGSaucF2AMDSdUCBMEOauBg4IowMa#scrollTo=Q_gXemVL-1Zt)
* Watermark removal with [vinthony/deep-blind-watermark-removal](https://github.com/vinthony/deep-blind-watermark-removal). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1pYY7byBjM-7aFIWk8HcF9nK_s6pqGwww?usp=sharing)
  * A more userfriendly version that does allow custom input with my fork [styler00dollar/Colab-deep-watermark](https://github.com/styler00dollar/Colab-deep-watermark). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-deep-watermark/blob/main/Colab-deep-watermark.ipynb)

## Video modification
* A Google Colab notebook set up for both conventional and machine learning-based video processing. This repo combines VapourSynth and ESRGAN and is located in [AlphaAtlas/VapourSynthColab](https://github.com/AlphaAtlas/VapourSynthColab). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlphaAtlas/VapourSynthColab/blob/master/VapourSynthColab.ipynb)
* Generates a talking face video from an image and an audio using [Rudrabha/LipGAN](https://github.com/Rudrabha/LipGAN).
  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/LipGAN.ipynb)
* Deoldify a video. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb)
* Decensoring mosaic with [HypoX64/DeepMosaics](https://github.com/HypoX64/DeepMosaics). My own colab fork is located in [styler00dollar/Colab-DeepMosaics](https://github.com/styler00dollar/Colab-DeepMosaics). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DeepMosaics/blob/master/Colab-DeepMosaics.ipynb)

## Image classification
* Image classification with [bentrevett/pytorch-image-classification](https://github.com/bentrevett/pytorch-image-classification). 
  * Multilayer Perceptron: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/1_mlp.ipynb)
  * LeNet: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/2_lenet.ipynb)
  * AlexNet: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/3_alexnet.ipynb)
  * VGG: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/4_vgg.ipynb)
  * ResNet: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/5_resnet.ipynb)
  * My very compact version of the ResNet notebook can be found inside [styler00dollar/Colab-image-classification](https://github.com/styler00dollar/Colab-image-classification): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-image-classification/blob/master/5_(small)_ResNet.ipynb)
* Using Autoencoders for unsupervised classification with [ardamavi/Unsupervised-Classification-with-Autoencoder](https://github.com/ardamavi/Unsupervised-Classification-with-Autoencoder). My fork [styler00dollar/Colab-UnsupervisedClassification](https://github.com/styler00dollar/Colab-UnsupervisedClassification) has some improvements and is usable with Colab. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-UnsupervisedClassification/blob/master/Colab-UnsupervisedClassification.ipynb)

## NLP
* Finetune GPT2 with [ak9250/gpt-2-colab](https://github.com/ak9250/gpt-2-colab/). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ak9250/gpt-2-colab/blob/master/GPT_2.ipynb)
* Using russian GPT2 and 3 (based on 2) with [sberbank-ai/ru-gpts](https://github.com/sberbank-ai/ru-gpts). 
  * Generation with ruGPT3large: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sberbank-ai/ru-gpts/blob/master/examples/ruGPT3_generation_example.ipynb#scrollTo=Y8DYR4d6_-w7)
  * Finetuning ruGPT3Small: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sberbank-ai/ru-gpts/blob/master/examples/Finetune_ruGPT3Small.ipynb#scrollTo=5vL07XFvsBBU)

## Misc
* Generate a human 3d model from a 2d picture with [facebookresearch/pifuhd](https://github.com/facebookresearch/pifuhd). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/11z58bl3meSzo6kFqkahMa35G5jmh2Wgt?usp=sharing)
* Turn a 2D image into a 3D video with [ai-coodinator/3D-Photo-Inpainting](https://github.com/ai-coodinator/3D-Photo-Inpainting) and [vt-vl-lab /3d-photo-inpainting](https://github.com/vt-vl-lab/3d-photo-inpainting). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ai-coodinator/3D-Photo-Inpainting/blob/master/3D_Photo_Inpainting.ipynb)
* Using Mask-RCNN and ESRGAN to detect barsand mosaic and depixelate content with [natethegreate/hent-AI](https://github.com/natethegreate/hent-AI). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/natethegreate/hent-AI/blob/master/hent_AI_COLAB_1.ipynb)
* Music Source Separation [sigsep/open-unmix-pytorch](https://github.com/sigsep/open-unmix-pytorch). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1mijF0zGWxN-KaxTnd0q6hayAlrID5fEQ)
* First Order Motion Model for Image Animation [AliaksandrSiarohin/first-order-model](https://github.com/AliaksandrSiarohin/first-order-model). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb)
* Official notebook of 3D Photography using Context-aware Layered Depth Inpainting [vt-vl-lab/3d-photo-inpainting](https://github.com/vt-vl-lab/3d-photo-inpainting). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz)
* [Image-GPT](https://github.com/openai/image-gpt) notebook. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Image_GPT_Sample_with_Conditioning.ipynb)
