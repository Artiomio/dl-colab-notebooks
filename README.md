Try out deep learning models online on Colab with a single click.

## Image Super-Resolution
* Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN). A combination of [xinntao/ESRGAN](https://github.com/xinntao/ESRGAN) and [ata4/esrgan-launcher](https://github.com/ata4/esrgan-launcher). My colab fork is located in [styler00dollar/Colab-ESRGAN](https://github.com/styler00dollar/Colab-ESRGAN). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-ESRGAN/blob/master/Colab-ESRGAN-(new&old-arch).ipynb)
* Deep Unfolding Network for Image Super-Resolution (USRNet). The original repositories are [cszn/USRNet](https://github.com/cszn/USRNet) and [cszn/KAIR](https://github.com/cszn/KAIR). My colab fork is located in [styler00dollar/Colab-USRNet](https://github.com/styler00dollar/Colab-USRNet). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-USRNet/blob/master/Colab-USRNet.ipynb)
* Image Super Resolution with [idealo/image-super-resolution](https://github.com/idealo/image-super-resolution). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/ISR_Prediction_Tutorial.ipynb)
* PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1-cyGV0FoSrHcQSVq3gKOymGTMt0g63Xc?usp=sharing#sandboxMode=true) or [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tg-bomze/Face-Depixelizer/blob/master/Face_Depixelizer_Eng.ipynb)

## Video Interpolation
* Depth-Aware Video Frame Interpolation (DAIN) using [baowenbo/DAIN](https://github.com/baowenbo/DAIN). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/baowenbo/DAIN/blob/master/Colab_DAIN.ipynb)
* High Quality Estimation of Multiple Intermediate Frames for Video Interpolation with [avinashpaliwal/Super-SloMo](https://github.com/avinashpaliwal/Super-SloMo). My colab fork is locaced in [styler00dollar/Colab-Super-SloMo](https://github.com/styler00dollar/Colab-Super-SloMo). My version: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-Super-SloMo/blob/master/Colab-Super-SloMo.ipynb) The original: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/SuperSloMo.ipynb)
* Featureflow can be found in [CM-BF/FeatureFlow](https://github.com/CM-BF/FeatureFlow). The colab was made by Mr. Anon. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1eg-ApEMzYRGUoacEtnxMWN9lESw_0E1C?usp=sharing)

## Video Super-Resolution and Interpolation
* Fast and Accurate One-Stage Space-Time Video Super-Resolution with [Mukosame/Zooming-Slow-Mo-CVPR-2020](https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020). My colab fork is located in [styler00dollar/Colab-Zooming-Slow-Mo](https://github.com/styler00dollar/Colab-Zooming-Slow-Mo). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-Zooming-Slow-Mo/blob/master/Colab-Zooming-Slow-Mo.ipynb)

## TTS
* LibriTTS trained multi speaker TTS demo using [NVIDIA/flowtron](https://github.com/NVIDIA/flowtron).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidia_Flowtron_Waveglow.ipynb)
* An English female voice ([LJSpeech](https://keithito.com/LJ-Speech-Dataset/)) demo using [Rayhane-mamah/Tacotron-2](https://github.com/Rayhane-mamah/Tacotron-2) and [r9y9/wavenet_vocoder](https://github.com/r9y9/wavenet_vocoder)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/Tacotron2_and_WaveNet_text_to_speech_demo.ipynb)
* A Mongolian male voice demo using [Rayhane-mamah/Tacotron-2](https://github.com/Rayhane-mamah/Tacotron-2) with the Griffin-Lim algorithm.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/mongolian-nlp/blob/master/misc/Tacotron_MongolianTTS.ipynb)  
* An English female voice ([LJSpeech](https://keithito.com/LJ-Speech-Dataset/)) demo using [tugstugi/pytorch-dc-tts](https://github.com/tugstugi/pytorch-dc-tts) with the Griffin-Lim algorithm.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/pytorch-dc-tts/blob/master/notebooks/EnglishTTS.ipynb)
* An English female voice ([LJSpeech](https://keithito.com/LJ-Speech-Dataset/)) demo using [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) (Tacotron + WaveRNN).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/fatchordWaveRNN.ipynb)
* An English female voice ([LJSpeech](https://keithito.com/LJ-Speech-Dataset/)) demo using [mozilla/TTS](https://github.com/mozilla/TTS) (Tacotron + WaveRNN).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/Mozilla_TTS_WaveRNN.ipynb)
* [NVIDIA/mellotron](https://github.com/NVIDIA/mellotron) notebook.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yhgon/mellotron/blob/master/inference_colab.ipynb)
* Voice clone demo using [CorentinJ/Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/RealTimeVoiceCloning.ipynb)
* Official [ESPnet](https://github.com/espnet/espnet) English/Chinese/Japanese TTS notebook. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb)
* Official [ForwardTacotron](https://github.com/as-ideas/ForwardTacotron) LJSpeech TTS notebook. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/as-ideas/ForwardTacotron/blob/master/notebooks/synthesize.ipynb)

## Speech Recognition
* [mozilla/DeepSpeech](https://github.com/mozilla/DeepSpeech) with LM on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/MozillaDeepSpeech.ipynb)
* Wav2Letter+ from [NVIDIA/OpenSeq2Seq](https://github.com/NVIDIA/OpenSeq2Seq.git) without LM on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidiaWav2LetterPlus.ipynb)
* Jasper from [NVIDIA/OpenSeq2Seq](https://github.com/NVIDIA/OpenSeq2Seq.git) without LM on Youtube videos. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidiaJasper.ipynb)
* QuartzNet from [NVIDIA/Nemo](https://github.com/NVIDIA/NeMo.git) without LM on Youtube videos. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidiaQuartzNet.ipynb)
* QuartzNet from [NVIDIA/Nemo](https://github.com/NVIDIA/NeMo.git) without LM with microphone. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidiaQuartzNetMic.ipynb)
* Official [ESPnet](https://github.com/espnet/espnet) Spanish->English speech translation notebook. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/st_demo.ipynb)

## Object Detection
* Tensorflow object detection: FasterRCNN+InceptionResNet and ssd+mobilenet.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb)
* Cascade RCNN demo using [open-mmlab/mmdetection](https://github.com/open-mmlab/mmdetection).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/Open_MMLab_Detection_Toolbox_Cascade_RCNN.ipynb)
* YOLO demo using [ayooshkathuria/pytorch-yolo-v3](https://github.com/ayooshkathuria/pytorch-yolo-v3).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/YOLOv3_PyTorch.ipynb)
* Object detection on Youtube videos using [amdegroot/ssd.pytorch](https://github.com/amdegroot/ssd.pytorch) (SSD300).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/SSD_Pytorch_Video.ipynb)  
* Mask RCNN demo using [matterport/Mask_RCNN](https://github.com/matterport/Mask_RCNN).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/Matterport_Mask_RCNN.ipynb)
* Mask RCNN demo using [Detectron](https://github.com/facebookresearch/Detectron).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/Detectron_MaskRCNN.ipynb)
* Official Mask RCNN demo from [Detectron2](https://github.com/facebookresearch/detectron2).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5)
* Mask RCNN demo from [torchvision](https://pytorch.org/docs/stable/torchvision/index.html).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/TorchvisionMaskRCNN.ipynb)
* CenterNet (Objects as Points) demo using [xingyizhou/CenterNet](https://github.com/xingyizhou/CenterNet).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/CenterNet_ObjectsAsPoints.ipynb)
* Official DE⫶TR demo notebook [facebookresearch/detr](https://github.com/facebookresearch/detr).
  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb)
* Official Google [EfficientDet](https://arxiv.org/abs/1911.09070) notebook.
  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/automl/blob/master/efficientdet/tutorial.ipynb)
* Test and train box-models from [Tensorflow detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) with [dctian/DeepPiCar](https://github.com/dctian/DeepPiCar). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dctian/DeepPiCar/blob/master/models/object_detection/code/tensorflow_traffic_sign_detection.ipynb)

## Segmentation
* For Mask RCNN, see [Object Detection](https://github.com/tugstugi/dl-colab-notebooks#object-detection).
* Semantic segmentation trained on [ADE20K](http://groups.csail.mit.edu/vision/datasets/ADE20K/) using [CSAILVision/semantic-segmentation-pytorch](https://github.com/CSAILVision/semantic-segmentation-pytorch).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/CSAILVision_SemanticSegmentation.ipynb)
* [DeepLabV3](https://arxiv.org/abs/1706.05587) from [torchvision](https://pytorch.org/docs/stable/torchvision/index.html).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/TorchvisionDeepLabV3.ipynb)
* Fast tracking and segmentation with [SiamMask](https://github.com/foolwood/SiamMask) on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/SiamMask.ipynb)
* Real-time semantic segmentation with [LightNet++](https://github.com/ansleliu/LightNetPlusPlus) on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/LightNetPlusPlus.ipynb)
* Real-time instance segmentation with [YOLACT](https://github.com/dbolya/yolact) on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/YOLACT.ipynb)
* Instance segmentation with [CenterMask](https://github.com/youngwanLEE/CenterMask/).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/CenterMask.ipynb)
* Train and test [Tensorflow detection model zoo mask models](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) with [TannerGilbert/Tensorflow-Object-Detection-API-train-custom-Mask-R-CNN-model](https://github.com/TannerGilbert/Tensorflow-Object-Detection-API-train-custom-Mask-R-CNN-model). There is also [a tutorial](https://gilberttanner.com/blog/train-a-mask-r-cnn-model-with-the-tensorflow-object-detection-api) dedicated to this repo. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/TannerGilbert/Tensorflow-Object-Detection-API-train-custom-Mask-R-CNN-model/blob/master/Tensorflow_Object_Detection_API_Instance_Segmentation_in_Google_Colab.ipynb)

## Object-Detection, Instance Segmentation, Person Keypoint Detection and Panoptic Segmentation
* Detectron2 is Facebook AI Research's next generation software system that implements state-of-the-art object detection algorithms. It is possible to train or test Object-Detection, Instance Segmentation, Person Keypoint Detection and Panoptic Segmentation with models that are available [here](https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md). [facebookresearch/detectron2](https://github.com/facebookresearch/detectron2) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5)

## Multi Object Tracking
* Pedestrian tracking using [ZQPei/deep_sort_pytorch](https://github.com/ZQPei/deep_sort_pytorch) (DeepSORT + YOLOv3).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/DeepSORT_YOLOv3.ipynb)  

## Pose Detection
* [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/OpenPose.ipynb)
* [AlphaPose](https://github.com/MVIG-SJTU/AlphaPose) on Youtube videos.
  * v0.2.0: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/AlphaPose.ipynb)
  * v0.3.0: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/AlphaPoseV0_3_0.ipynb)
* [DensePose](https://github.com/facebookresearch/DensePose) demo notebook.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/DensePose.ipynb)
* HRNet using [lxy5513/hrnet](https://github.com/lxy5513/hrnet) on Youtube videos.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/HRNet_lxy5513.ipynb)
* Keypoint R-CNN from [torchvision](https://pytorch.org/docs/stable/torchvision/index.html).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/TorchvisionPersonKeypoint.ipynb)

## Scene Text Detection
* [PixelLink](https://github.com/ZJULearning/pixel_link) demo notebook.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/PixelLink.ipynb)
* Scene text detection using [argman/EAST](https://github.com/argman/EAST).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/EAST.ipynb)
* Scene text detection using [CRAFT-pytorch](https://github.com/clovaai/CRAFT-pytorch).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/CRAFT.ipynb)

## Image generation
* BigGAN [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096).
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb)  
* Style-based GAN architecture (StyleGAN2) can be found in [NVlabs/stylegan2](https://github.com/NVlabs/stylegan2). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/parthsuresh/stylegan2-colab/blob/master/StyleGAN2_Google_Colab.ipynb)
* Style-based GAN architecture (StyleGAN2) with anime face generation. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Pv8OIFlonha4KeYyY2oEFaK4mG-alaWF#scrollTo=q8VnyjDhiBQY&forceEdit=true&sandboxMode=true)

## Image modification
* [DeOldify](https://github.com/jantic/DeOldify): A Deep Learning based project for colorizing and restoring old images.
  * deoldify an image [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb)
* Decensoring Hentai with Deep Neural Networks. The original repo is [deeppomf/DeepCreamPy](https://github.com/deeppomf/DeepCreamPy) and my fork is located in [styler00dollar/Colab-DeepCreamPy](https://github.com/styler00dollar/Colab-DeepCreamPy). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DeepCreamPy/blob/master/Colab-DeepCreamPy-2.0.ipynb)
* Deblurring pictures with [TAMU-VITA/DeblurGANv2](https://github.com/TAMU-VITA/DeblurGANv2). My own fork is located in [styler00dollar/Colab-DeblurGANv2](https://github.com/styler00dollar/Colab-DeblurGANv2). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DeblurGANv2/blob/master/Colab-DeblurGANv2.ipynb)
* Deblurring pictures with [SeungjunNah/DeepDeblur-PyTorch](https://github.com/SeungjunNah/DeepDeblur-PyTorch). My own fork is located in [styler00dollar/Colab-DeepDeblur](https://github.com/styler00dollar/Colab-DeepDeblur). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DeepDeblur/blob/master/Colab-DeepDeblur.ipynb)

## Video modification
* A Google Colab notebook set up for both conventional and machine learning-based video processing. This repo combines VapourSynth and ESRGAN and is located in [AlphaAtlas/VapourSynthColab](https://github.com/AlphaAtlas/VapourSynthColab). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlphaAtlas/VapourSynthColab/blob/master/VapourSynthColab.ipynb)
* Generates a talking face video from an image and an audio using [Rudrabha/LipGAN](https://github.com/Rudrabha/LipGAN).
  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/LipGAN.ipynb)
* Deoldify a video. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb)
* Decensoring mosaic with [HypoX64/DeepMosaics](https://github.com/HypoX64/DeepMosaics). My own colab fork is located in [styler00dollar/Colab-DeepMosaics](https://github.com/styler00dollar/Colab-DeepMosaics). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/styler00dollar/Colab-DeepMosaics/blob/master/Colab-DeepMosaics.ipynb)

## Misc
* Using Mask-RCNN and ESRGAN to detect barsand mosaic and depixelate content with [natethegreate/hent-AI](https://github.com/natethegreate/hent-AI). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/natethegreate/hent-AI/blob/master/hent_AI_COLAB_1.ipynb)
* Finetune GPT2 [ak9250/gpt-2-colab](https://github.com/ak9250/gpt-2-colab/). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ak9250/gpt-2-colab/blob/master/GPT_2.ipynb)
* Music Source Separation [sigsep/open-unmix-pytorch](https://github.com/sigsep/open-unmix-pytorch). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1mijF0zGWxN-KaxTnd0q6hayAlrID5fEQ)
* First Order Motion Model for Image Animation [AliaksandrSiarohin/first-order-model](https://github.com/AliaksandrSiarohin/first-order-model). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb)
* Official notebook of 3D Photography using Context-aware Layered Depth Inpainting [vt-vl-lab/3d-photo-inpainting](https://github.com/vt-vl-lab/3d-photo-inpainting). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz)
* [Image-GPT](https://github.com/openai/image-gpt) notebook. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/apeguero1/image-gpt/blob/master/Image_GPT_Sample_with_Conditioning.ipynb)
